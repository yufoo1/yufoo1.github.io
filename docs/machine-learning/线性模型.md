---
sidebar_position: 2
---
# 线性模型

## 基本形式

给定由$d$个属性描述的示例$x=(x_1;x_2;\cdots;x_d)$，其中$x_i$是$x$在第$i$个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即$f(x)=w_1x_1+w_2x_2+\cdots+w_dx_d+b$，一般用向量形式写成$f(x)=w^Tx+b$，其中$w=(w_1;w_2;\cdots;w_d)$。$w$和$b$学得之后，模型就得以确定。

## 线性回归

### 二元线性回归

基于均方误差最小化来进行模型求解的方法称为“最小二乘法”，即找到$w$和$b$，使得$\sum\limits_{i=1}^m(f(x_i)-y_i)^2=\sum\limits_{i=1}^m(y_i-wx_i-b)^2$达到最小值。

求解$w$和$b$使$E_{w, b}=\sum\limits_{i=1}^m(y_i-wx_i-b)^2$最小化的过程，称为线性回归模型的最小二乘“参数估计”。令$\frac{\partial E_{w, b}}{\partial w}=0$且$\frac{\partial E_{w, b}}{\partial b}=0$解得$w$和$b$最优解的闭式解$w=\frac{\sum\limits_{i=1}^my_i(x_i-\overline{x})}{\sum\limits_{i=1}^mx_i^2-\frac{1}{m}(\sum\limits_{i=1}^mx_i)^2}$，$b=\frac{1}{m}\sum\limits_{i=1}^m(y_i-wx_i)$，其中$\overline{x}=\frac{1}{m}\sum\limits_{i=1}^mx_i$为$x$的均值。

### 多元线性回归

”多元线性回归“可类似利用最小二乘法来对$w$和$b$进行估计。

令$\hat{w}=(w; b)$，并将数据集$D$表示为一个$m×(d+1)$大小的矩阵$X$，其中每行对应于一个示例，该行前$d$个元素对应于示例的$d$个属性值，最后一个元素横置为1。再把标记也写成向量形式$y=(y_1;y_2;\cdots;y_m)$，令$E_\hat{w}=(y-X\hat{w})^T(y-X\hat{w})$，对$\hat{w}$求导得到$\frac{\partial E_{\hat{w}}}{\partial\hat{w}}=2X^T(X\hat{w}-y)$。

当$X^TX$为满秩矩阵或正定矩阵时，令$\frac{\partial E_{\hat{w}}}{\partial\hat{w}}=0$可得$\hat{w}^*=(X^TX)^{-1}X^Ty$。令$\hat{x}_i=(x_i;1)$，则最终学得的多元线性回归模型为$f(\hat{x}_i)=\hat{x}_i^T(X^TX)^{-1}X^Ty$。

当$X^TX$不是满秩矩阵时，可解出多个$\hat{w}$，使得均方误差最小化。选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化项。

### 广义线性模型

考虑单调可微函数$g(\cdot)$，令$y=g^{-1}(w^Tx+b)$，这样得到的模型称为"广义线性模型"，其中函数$g(\cdot)$称为”联系函数“。对数线性回归是广义线性模型在$g(\cdot)=ln(\cdot)$时的特例。

广义线性模型的参数估计常通过加权最小二乘法或极大似然法进行。

## 对数几率回归

对数几率回归本质为一种分类学习方法，即通过一个单调可微函数将分类任务的真实标记$y$与线性回归模型的预测值联系起来。常用的函数有对数几率函数$y=\frac{1}{1+e^{-z}}$，它不仅可以预测出”类别“，还可得到近似概率预测。

## 线性判别分析

$LDA$的思想是给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。

值得一提的是，$LDA$可从贝叶斯决策理论的角度来阐释，并可证明当两类数据同先验、满足高斯分布且协方差相等时，$LDA$可达到最优分类，同时$LDA$可推广到多分类任务中。

## 多分类任务

多分类学习的基本思路是”拆解法“，即将多分类任务拆为若干个二分类任务求解，因此多分类任务求解的关键在于如何对多分类任务进行拆分，以及如何对多个分类器进行集成。最经典的拆分策略有三种：”一对一“、”一对其余“、”多对多“。

### OvO

$OvO$将数据集的$N$个类别两两配对，从而产生$\frac{N(N-1)}{2}$个二分类任务，于是我们将得到$\frac{N(N-1)}{2}$个分类结果，最终结果可通过投票产生：即把被预测得最多的类别作为最终分类结果。

### OvR

$OvR$是每次将一个类的样例作为正例、所有其他类的样例作为反例来训练$N$个分类器。在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。

### MvM

$MvM$是每次将若干个类作为正类，若干个其他类作为反类。$MvM$的正、反类构造必须有特殊的设计，一种最常用的$MvM$技术是”纠错输出码“。

## 类别不平衡问题

类别不平衡是指分类任务中不同类别的训练样例数目差别很大的情况。类别不平衡学习的一个基本策略——”再缩放“是令$\frac{y'}{1-y'}=\frac{y}{1-y}×\frac{m^-}{m^+}$。

”训练集是真实样本总体的无偏采样“这个假设往往不成立，也就是说，我们未必能有效地基于训练集观测几率来推断出真实几率。现有技术大体上有三类做法：第一类是直接对训练集里的样例进行”欠采样“，即去除一些示例使得正、反例数目接近，然后再进行学习；第二类是对训练集里的样例进行”过采样“，即增加一些示例使得正、反数目接近，然后再进行学习；第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将$\frac{y'}{1-y'}=\frac{y}{1-y}×\frac{m^-}{m^+}$嵌入到其决策过程中，称为”阈值移动“。

$SMOTE$是过采样法的代表性算法，其通过对训练集里的正例进行插值来产生额外的正例。

$EasyEnsemble$是欠采样的代表性算法，其利用集成学习机制，将反例划分为若干个集合供不同学习器学习，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息。

值得一提的是，”再缩放“也是”代价敏感学习“的基础。
