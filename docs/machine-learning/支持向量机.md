---
sidebar_position: 5
---
# 支持向量机

## 间隔与支持向量

在样本空间中，划分超平面可通过$w^Tx+b=0$来描述，其中$w=(w_1;w_2;\cdots;w_d)$为法向量，决定了超平面的方向；$b$为位移项，决定了超平面与原点的距离。

样本空间中任意点$x$到超平面$(w,b)$的距离可写为$r=\frac{|w^Tx+b|}{||w||}$。距离超平面最近的训练样本点使$\begin{cases} w^Tx_i+b\ge+1& {y_i=+1}\\w^Tx_i+b\le-1&{y_i=-1}\end{cases}$ 的等号成立，它们被称为“支持向量”，两个异类支持向量到超平面的距离之和为$\gamma=\frac{2}{||w||}$，它被称为“间隔”。

### 支持向量机基本型

$\mathop{min}\limits_{w,b}\frac{1}{2}||w||^2$，$s.t.\quad y_i(w^Tx_i+b)\ge1$，$i=1,2,\cdots,m$。

支持向量机训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。

### SMO算法

$SMO$算法是人们提出用于解决$\mathop{max}\limits_\alpha\sum\limits_{i=1}^m\alpha_i-\frac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j^T$，$s.t.\quad\sum\limits_{i=1}^m\alpha_iy_i=0,\alpha_i\ge0,i=1,2,\cdots,m$（\*）的高效算法，其基本思路是先固定$\alpha_i$之外的所有参数，然后求$\alpha_i$上的极值。因此在参数初始化后，$SMO$不断执行如下两个步骤直至收敛：选取一对需更新的变量$\alpha_i$和$\alpha_j$；固定$\alpha_i$和$\alpha_j$以外的参数，求解（*）获得更新后的$\alpha_i$和$\alpha_j$。同时，$SMO$采用了一个启发式选择调整的参数：使选取的两变量所对应样本之间的间隔最大。

偏移项$b$通过$b=\frac{1}{|S|}\sum\limits_{s\in S}(\frac{1}{y_s}-\sum\limits_{i\in S}\alpha_iy_ix_i^Tx_s)$求解。

## 核函数

对于线性不可分的问题，我们可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。令$\phi(x)$表示将$x$映射后的特征向量，在特征空间中划分超平面所对应的模型可表示为$f(x)=w^T\phi(x)+b$，其中$w$和$b$是模型参数。

### 核技巧

设$k(x_i,x_j)=<\phi(x_i),\phi(x_j)>=\phi(x_i)^T\phi(x_j)$，则我们只需求解$\mathop{max}\limits_\alpha\sum\limits_{i=1}^m\alpha_i-\frac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m\alpha_i\alpha_jy_iy_jk(x_i,x_j)$，$s.t.\quad\sum\limits_{i=1}^m\alpha_iy_i=0,\alpha_i\ge0,i=1,2,\cdots,m$。求解后得到$f(x)=w^T\phi(x)+b=\sum\limits_{i=1}^m\alpha_iy_i\phi(x_i)^T\phi(x)+b=\sum\limits_{i=1}^m\alpha_iy_ik(x,x_i)+b$，其中$k(·,·)$就是"核函数"。

### 核函数定理

令$X$为输入空间，$k(·,·)$是定义在$X×X$上的对称函数，$则k$是核函数当且仅当对于任意数据$D=\{x_1,x_2,\cdots,x_m\}$，“核矩阵”$K$总是半正定的，其中核矩阵$K=\begin{bmatrix}k(x_1,x_1)&\cdots&k(x_1,x_j)&\cdots&k(x_1,x_m)\\\vdots&\ddots&\vdots&\ddots&\vdots\\k(x_i,x_1)&\cdots&k(x_i,x_j)&\cdots&k(x_i,x_m)\\\vdots&\ddots&\vdots&\ddots&\vdots\\k(x_m,x_1)&\cdots&k(x_m,x_j)&\cdots&k(x_m,x_m)\end{bmatrix}$。

上述定理表明，只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。

### 常用核函数

|    名称     |                                表达式                                 |                    参数                     |
| :---------: |:------------------------------------------------------------------:| :-----------------------------------------: |
|    线性     |                       $k(x_i,x_j)=x_i^Tx_j$                        |                                             |
|  多项式核   |                     $k(x_i,x_j)=(x_i^Tx_j)^d$                      |            $d\ge0$为多项式的次数            |
|   高斯核    |     $k(x_i,x_j)=e^{-\frac{\lvert x_i-x_j\rvert^2}{2\sigma^2}}$     |          $\sigma>0$为高斯核的带宽           |
| 拉普拉斯核  | $k(x_i,x_j)=e^{-\frac{          \lvert x_i-x_j\rvert}{2\sigma^2}}$ |                 $\sigma>0$                  |
| $Sigmoid$核 |              $k(x_i,x_j)=tanh(\beta x_i^Tx_j+\theta)$              | $tanh$为双曲正切函数，$\beta>0$，$\theta<0$ |

若$k1$和$k2$为核函数，则对于任意正数$\gamma_1$、$\gamma_2$，其线性组合$\gamma_1k_1+\gamma_2k_2$也是核函数。

若$k1$和$k2$为核函数，则核函数的直积$k_1\otimes k_2(x,z)=k_1(x,z)k_2(x,z)$也是核函数。

若$k_1$为核函数，则对于任意函数$g(x)$，$k(x,z)=g(x)k_1(x,z)g(z)$也是核函数。

## 软间隔与正则化

为避免过拟合，我们引入“软间隔”的概念，其允许支持向量在一些样本上出错。

在最大化间隔的同时，不满足约束的样本应尽可能少，于是优化目标可写为$\mathop{min}\limits_{w,b}\frac{1}{2}||w||^2+C\sum\limits_{i+1}^m\mathop{l}_{0/1}(y_i(w^Tx_i+b)-1)$，其中$C>0$是一个常数，$l_{0/1}(z)$是“0/1损失函数”$\begin{cases}1,&if\quad z<0\\0,&if\quad otherwise\end{cases}$。

### 常用替代损失函数

|    名称     |           表达式           |
| :---------: | :------------------------: |
| $hinge$损失 | $l_{hinge}(z)=max(0,1-z)$  |
|  指数损失   |    $l_{exp}(z)=e^{-z}$     |
|  对数损失   | $l_{log}(z)=log(1+e^{-z})$ |

### 松弛变量

可引用“松弛变量”$\xi\ge0$，$\mathop{min}\limits_{w,b,\xi}\frac{1}{2}||w||^2+C\sum\limits_{i=1}^m\xi_i$，$s.t.\quad y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,m$，这就是常用的“软间隔支持向量机”。其中每个样本都有一个对应的松弛变量，用以表征该样本不满足约束的程度。

## 支持向量回归

支持向量回归假设我们能容忍$f(x)$与$y$之间最多有$\epsilon$的偏差，即仅当$f(x)$与$y$之间的差别绝对值大于$\epsilon$时才计算损失。

## 核方法

基于核函数的学习方法，统称为“核方法”。

### 表示定理

令$H$为核函数$k$对应的再生核希尔伯特空间，$||h||_H$表示$H$空间中关于$h$的范数，对于任意单调递增函数$\Omega:[0,\infty]\to R$和任意非负损失$l:R^m\to[0,\infty]$，优化问题$\mathop{min}\limits_{h\in H}F(h)=\Omega(||h||_H)+l(h(x_1),h(x_2),\cdots,h(x_m))$的解总可写为$h*(x)=\sum\limits_{i=1}^m\alpha_ik(x,x_i)$。

表示定理对损失函数没有限制，对正则化项$\Omega$仅要求单挑递增。
