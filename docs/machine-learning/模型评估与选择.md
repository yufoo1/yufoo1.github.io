---
sidebar_position: 1
---
# 模型评估与选择

## 经验误差与过拟合

错误率：$E=\frac{a}{m}$

精度：$(1-\frac{a}{m})×100\%$

误差：学习器的实际预测输出与样本的真实输出之间的差异。

训练误差：学习器在训练集上的误差。

泛化误差：学习器在新样本上的误差。

过拟合：将训练样本本身的一些特点当作了所有潜在样本都会具有的一般性质，导致泛化性能下降的现象。

欠拟合：指对训练样本的一般性质尚未学好。

## 评估方法

### 留出法

将数据集$D$划分为两个互斥的集合，其中一个集合作为训练集$S$，另一个作为测试集$T$。在$S$上训练出模型后，用$T$来评估其测试误差，作为对泛化误差的估计。

需要注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。

另一个需注意的问题是，即使在给定训练/测试集的样本比例后，仍存在多种划分方式对初始数据集$D$进行分割。这些不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差别，故使用留出法时一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。

### 交叉验证法

将数据集$D$划分为$k$个大小相似的互斥子集，每个子集$D_i$都尽可能保持数据分布的一致性，然后每次用$k-1$个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得$k$组训练/测试集，从而可进行$k$次训练和测试，最终返回的是这$k$个测试结果的均值。

与留出法相似，将数据集$D$划分为$k$个子集同样存在多种划分方式，为减小因样本划分不同而引入的差别，$k$折交叉验证通常要随机使用不同的划分重复$p$次，最终的评估结果是这$p$次$k$折交叉验证结果的均值。

### 自助法

给定包含$m$个样本的数据集$D$，对它进行采样产生数据集$D'$：每次随机从$D$中挑选一个样本，将其拷贝放入$D'$，然后再将该样本放回初始数据集$D$中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行$m$次后，就得到了包含$m$个样本的数据集$D'$，这就是自主采样的结果。通过自主采样，初始数据集$D$中约有36.8%的样本未出现在采样数据集$D'$中，于是我们可将$D'$用作训练集，$D\backslash D'$用作测试集。

## 性能度量

### 回归

####  均方误差

回归任务最常用的性能度量是“均方误差”：$E(f;D)=\frac{1}{m}\sum\limits_{i=1}^m(f(x_i)-y_i)^2$。

更一般的，对于数据分布$D$和概率密度函数$p(·)$，均方误差可描述为：$E(f;D)=\int_{x\sim D}(f(x_i)-y_i)^2p(x)dx$。

### 分类

#### 错误率和精度

分类任务中常用错误率和精度来进行性能度量，其既适用于二分类任务，也适用于多分类任务。

更一般的，对于数据分布$D$和概率密度函数$p(·)$，错误率可描述为：$E(f;D)=\int_{x\sim D}I(f(x)\neq y)p(x)dx$；精度可描述为$acc(f;D)=\int_{x\sim D}I(f(x)= y)p(x)dx=1-E(f;D)$。

#### 查准率、查全率和P-R曲线

对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例、假正例、真反例、假反例四种情形，令$TP$、$FP$、$TN$、$FN$分别表示其对应的样例数。查准率$P$定义为：$P=\frac{TP}{TP+FP}$；查全率$R$定义为：$R=\frac{TP}{TP+FN}$。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。

在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本。按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率。以查准率为纵轴、查全率为横轴作图，就得到了查准率-查全率曲线，简称“$P-R$曲线”。

#### 平衡点

平衡点是一个综合查准率、查全率的性能度量，它是“查准率=查全率时的取值”。

#### F1度量和$F_\beta$度量

$F1$是基于查准率与查全率的调和平均定义的：$F1=\frac{2×P×R}{P+R}=\frac{2×TP}{样例总数+TP-TN}$。

$F1$度量的一般形式——$F_\beta$能让我们表达出查准率/查全率的不同偏好，它定义为：$F_{\beta}=\frac{1+\beta^2×P×R}{(\beta^2×P)+R}$。其中$\beta>0$度量了查全率对查准率的相对重要性：$\beta=1$时退化为标准的$F1$；$\beta>1$时查全率有更大影响；$\beta<1$时查准率有更大影响。

#### 宏查准率、宏查全率和宏F1

当进行多次训练/测试，或是在多个数据集上进行训练/测试时，我们会得到多个二分类混淆矩阵，并希望依次估计算法的“全局”性能；甚或是执行多分类任务，每两两类别的组合都对应一个混淆矩阵，我们希望在$n$个二分类混淆矩阵上综合考察查准率和查全率。一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率，再计算平均值。

宏查准率：$macro-P=\frac{1}{n}\sum\limits_{i=1}^nP_i$；宏查全率：$macro-R=\frac{1}{n}\sum\limits_{i=1}^nR_i$；宏$F1$：$macro-F1=\frac{2×macro-P×macro-R}{macro-P+macro-R}$。

#### 微查准率、微查全率和微F1

还可先将各混淆矩阵的对应元素进行平均，再基于这些平均值计算出“微查准率”、“微查全率”和“微$F1$”。

微查准率：$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$；宏查全率：；$micro-R=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$；宏$F1$：$micro-F1=\frac{2×micro-P×micro-R}{micro-P+micro-R}$。

#### ROC曲线

$ROC$曲线的纵轴是“真正例率”，横轴是“假正例率”，两者分别定义为：$TPR=\frac{TP}{TP+FN}$；$FP=\frac{FP}{TN+FP}$。

进行学习器的比较时，与$P-R$图相似，若一个学习器的$ROC$曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者；若两个学习器的$ROC$曲线发生交叉，则难以一般性地断言两者孰优孰劣，较为合理的判据是比较$ROC$曲线下的面基，即$AUC$。$AUC$可估算为$AUC=\frac{1}{2}\sum\limits_{i=1}^{m-1}(x_{i+1}-x_i)·(y_i+y_{i+1})$。

#### 代价敏感错误率与代价曲线

现实任务中，不同类型的错误所造成的后果不同，在非均等代价下，我们希望最小化“总体代价”，则“代价敏感”错误率为$E(f;D;cost)=\frac{1}{m}(\sum\limits_{x_i\in D^+}I(f(x_i)\neq y_i)×cost_{01}+\sum\limits_{x_i\in D^-}I(f(x_i)\neq y_i)×cost_{10})$。

在非均等代价下，$ROC$曲线不能直接反应出学习器的期望总体代价，而“代价曲线”则可达到改目的。代价曲线图的横轴是取值为$[0, 1]$的正例概率代价：$P(+)cost=\frac{p×cost_{01}}{p×cost_{01}+(1-p)×cost_{10}}$。其中$p$是样例为正例的概率；纵轴是取值为$[0, 1]$的归一化代价：$cost_{norm}=\frac{FNR×p×cost_{01}+FPR×(1-p)×cost_{10}}{p×cost_{01}+(1-p)×cost_{10}}$。其中$FNR=1-TPR$。

## 比较检验

### 单学习器泛化性能假设检验

#### 二项检验

一次留出法可通过“二项检验”进行假设检验。

#### t检验

通过多次重复留出法或是交叉验证法等进行多次训练/测试，这样会得到多个测试错误率，此时可使用“t检验”。

### 多学习器泛化性能假设检验

#### 交叉验证t检验

对两个学习器$A$和$B$，若我们使用$k$折交叉验证法得到的测试错误率分别为$\epsilon_1^A$，$\epsilon_2^A$，$\cdots$，$\epsilon_k^A$和$\epsilon_1^B$，$\epsilon_2^B$，$\cdots$，$\epsilon_k^B$，其中$\epsilon_i^A$和$\epsilon_i^B$是在相同的第$i$折训练/测试集上得到的结果，则可用$k$折交叉验证“成对$t$检验”来进行比较检验。

#### McNemar检验

若我们做的假设是两学习器性能相同，则可使用$McNemar$检验。

#### Friedman检验

当有多个算法参与比较时，一种做法是在每个数据集上分别列出两两比较的结果，而在两两比较时可使用前述方法；另一种方法更为直接，即使用基于算法排序的$Friedman$检验。

#### Nemenyi后续检验

若“所有算法的性能相同”这个假设被拒绝，则说明算法的性能显著不同。这时需进行“后续检验”来进一步区分各算法。常用的有$Nemenyi$后续检验。

## 偏差与方差

“偏差-方差分解”是解释学习算法泛化性能的一种重要工具。$E(f;D)=bias^2(x)+var(x)+\epsilon^2$，也就是说，泛化误差可分解为偏差、方差与噪声之和。其中偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

一般来说，偏差与方差是有冲突的，这称为偏差-方差窘境。
