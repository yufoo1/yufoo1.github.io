---
sidebar_position: 4
---
# 决策树

## 划分选择

### 信息增益

”信息熵“是度量样本集合纯度最常用的一种指标，假定当前样本集合$D$中第$k$类样本所占的比例为$p_k(k=1,2,\cdots,|\gamma|)$，则$D$的信息熵定义为$Ent(D)=-\sum\limits_{k=1}^{|\gamma|}p_klog_2p_k$。$Ent(D)$的值越小，则$D$的纯度越高。

假定离散属性$a$有$V$个可能的取值${a^1,a^2,\cdots,a^V}$，若使用$a$来对样本集$D$进行划分，则会产生$V$个分支结点，其中第$v$个分支节点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记为$D^v$。我们可根据$Ent(D)=-\sum\limits_{k=1}^{|\gamma|}p_klog_2p_k$计算出$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$|D^v|/|D|$，即样本数越多的分支结点的影响越大，于是可计算出属性$a$对样本集$D$进行划分所获得的”信息增益“：$Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$。

### ID3决策树算法

一般而言，信息增益越大，则意味着使用属性$a$来进行划分所获得的”纯度提升“越大。著名的$ID3$决策树学习算法就是以信息增益为准则来选择划分属性。

### C4.5决策树算法

信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，$C4.5$决策树算法不直接使用信息增益，而是使用”增益率“来选择最优划分属性。

增益率定义为：$Gain\_ratie(D,a)=\frac{Gain(D,a)}{IV(a)}$，其中$IV(a)=-\sum\limits_{v=1}^V\frac{|D^v|}{D}log_2\frac{|D^v|}{|D|}$称为属性$a$的”固有值“。需要注意的是，增益率准则对可取数目较少的属性有所偏好，因此$C4.5$算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式算法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

### CART决策树

$CART$决策树使用”基尼指数“来选择划分属性，数据集$D$的纯度可用基尼值来度量：$Gini(D)=\sum\limits_{k=1}^{|\gamma|}\sum\limits_{k'\neq k}p_kp_{k'}=1-\sum\limits_{k=1}^{|\gamma|}p_k^2$；属性$a$的基尼指数定义为$Gini+index(D,a)=\sum\limits_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$。于是，我们在侯选属性集合$A$中，选择那个使得划分后基尼指数最小的属性作为最优划分属性。

## 剪枝处理

剪枝是决策树学习算法对付”过拟合“的主要手段。决策树剪枝的基本策略有”预剪枝“和”后剪枝“，预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶节点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点对应地子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点。

### 预剪枝

预剪枝基于”贪心“本质禁止某些分支展开，给预剪枝决策树带来了欠拟合的风险。

### 后剪枝

一般情形下，后剪枝决策树地欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中地所有非叶节点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。

## 连续与缺失值

### 连续值处理

由于连续属性的可取值数目不再有限，因此不能直接根据连续属性的可取值来对结点进行划分，此时可使用连续属性离散化技术。最简单的策略是采用二分法对连续属性进行处理，这正是$C4.5$决策树算法中采用的机制。其中$Gain(D,a)=\mathop{max}\limits_{t\in T_a}Gain(D,a,t)=\mathop{max}\limits_{t\in T_a}Ent(D)-\sum\limits_{\lambda\in\{-,+\}}\frac{|D_t^\lambda|}{|D|}Ent(D_t^\lambda)$，其中$Gain(D,a,t)$是样本集$D$基于划分点$t$二分后的信息增益。于是，我们就可选择使$Gain(D,a,t)$最大化的划分点。

需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性。

### 缺失值处理

给定训练集$D$和属性$a$，令$\tilde{D}$表示$D$中在属性$a$上没有缺失值的样本子集，关于如何在属性值缺失的情况下进行属性选择，我们可通过$\tilde{D}$来判断属性$a$的优劣。假定属性$a$有$V$个可取值$\{a^1,a^2,\cdots,a^V\}$，令$\tilde{D}^v$表示$\tilde{D}$中在属性$a$上取值为$a^v$的样本子集，$\tilde{D}_k$表示$\tilde{D}$中属于第$k$类$(k=1,2,\cdots,|\gamma|)$的样本子集，则显然有$\tilde{D}=\mathop{\cup}\limits_{k=1}^{|\gamma|}\tilde{D}_k$，$\tilde{D}=\mathop{\cup}\limits_{v=1}^{V}\tilde{D}^v$。假定我们为每个样本$x$赋予一个权重$w_x$，并定义$\rho=\frac{\sum\limits_{x\in\tilde{D}}w_x}{\sum\limits_{x\in D}w_x}$；$\tilde{p}_k=\frac{\sum\limits_{x\in\tilde{D}_k}w_x}{\sum\limits_{x\in\tilde{D}}w_x}(1\le k\le|\gamma|)$；$\tilde{r}_v=\frac{\sum\limits_{x\in\tilde{D}^v}w_x}{\sum\limits_{x\in\tilde{D}}w_x}(1\le v\le V)$。对属性$a$，$\rho$表示无缺失值样本所占的比例，$\tilde{p}_k$表示无缺失值样本中第$k$类所占的比例，$\tilde{r}_v$则表示无缺失值样本中在属性$a$上取值$a^v$的样本所占的比例。显然$\sum\limits_{k=1}^{|\gamma|}\tilde{p}_k=1$，$\sum\limits_{v=1}^{V}\tilde{r}_v=1$。

基于上述定义，我们可将信息增益的计算式推广为$Gain(D,a)=\rho×Gain(\tilde{D},a)$。

关于给定划分属性，若样本在该属性上的值已知，则将其划入与其取值对应的子节点，且样本权值在子结点中保持为$w_x$。若样本在划分属性上的取值未知，则将其同时划入所有子结点，且样本权值与属性值$a^v$对应的子结点中调整为$\tilde{r}_v\cdot w_x$。

## 多变量决策树

若我们把每个属性视为坐标空间中的一个坐标轴，则$d$个属性描述的样本就对应了$d$维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界。决策树所形成的分类边界有一个明显的特点：轴平行，即它的分类边界由若干个与坐标轴平行的分段组成。

”多变量决策树“能实现”斜划分“甚至更复杂划分的决策树，以实现斜划分的多变量决策树为例，在此类决策树中，非叶节点不再是仅对某个属性，而是对属性的线性组合进行测试，即每个非叶节点是一个形如$\sum\limits_{i=1}^dw_ia_i=t$的线性分类器，其中$w_i$是属性$a_i$的权重，$w_i$和$t$可在该结点所含的样本集和属性集上学得。多决策树算法主要有$OC1$和”感知机树“，$OC1$先贪心地寻找每个属性的最优权值，在局部优化的基础上再对分类边界进行随机扰动以试图找到更好的边界；”感知机树“在决策树的每个叶节点上训练一个感知机，亦可直接在叶节点上嵌入多层神经网络。像$ID4$、$ID5R$、$ITI$等决策树算法可进行“增量学习”，即在接收到新样本后可对已学得的模型进行调整。

