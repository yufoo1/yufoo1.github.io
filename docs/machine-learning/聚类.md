---
sidebar_position: 3
---
# 聚类

## 聚类任务

“聚类”应用于“无监督学习”中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。

假定样本集$D=\{x_1,x_2,\cdots,x_m\}$包含$m$个无标记样本，每个样本$x_i=(x_{i1};x_{i2};\cdots;x_{in})$是一个$n$维特征向量，则聚类算法将样本集$D$划分为$k$个不相交的簇$\{C_l|l=1,2,\cdots,k\}$，其中$C_{l'}\cap_{l'\neq l}C_l=\varnothing$且$D=\cup_{l=1}^kC_l$。相应地，我们用$\lambda_j\in\{1,2,\cdots,k\}$表示样本$x_j$的”簇标记“，即$x_j\in C_{\lambda_j}$。于是，聚类的结果可用包含$m$个元素的簇标记向量$\lambda=(\lambda_1;\lambda_2;\cdots;\lambda_m)$表示。

## 性能度量

对数据集$D=\{x_1,x_2,\cdots,x_m\}$，假定通过聚类给出的簇划分为$C=\{C_1,C_2,\cdots,C_k\}$，参考模型给出的簇划分为$C^*=\{C_1^*,C_2^*,\cdots,C_s^*\}$。相应地，令$\lambda$与$\lambda^*$分别表示与$C$和$C^*$对应的簇标记向量。我们将样本两两配对考虑，定义$a=|SS|,SS=\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda^*_i=\lambda^*_j,i<j\}$，$b=|SD|,SD=\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda^*_i\neq\lambda^*_j,i<j\}$，$c=|DS|,DS=\{(x_i,x_j)|\lambda_i\neq\lambda_j,\lambda^*_i=\lambda^*_j,i<j\}$，$d=|DD|,DD=\{(x_i,x_j)|\lambda_i\neq\lambda_j,\lambda^*_i\neq\lambda^*_j,i<j\}$。

**Jaccard系数**：$JC=\frac{a}{a+b+c}$

**FM指数**：$FMI=\sqrt{\frac{a}{a+b}·\frac{a}{a+c}}$

**Rand指数**：$RI=\frac{2(a+d)}{m(m-1)}$

上述性能度量的结果值均在$[0,1]$区间，值越大越好。

考虑聚类结果的簇划分$C=\{C_1,C_2,\cdots,C_k\}$，定义$avg(C)=\frac{2}{|C|(|C|-1)}\sum_{1\le i<j\le|C|}dist(x_i;x_j)$，$diam(C)=max_{1\le i<j\le|C|dist(x_i,x_j)}$，$d_{min}(C_i,C_j)=min_{x_i\in C_i,x_j\in C_j}dis(x_i,x_j)$，$d_{cen}(C_i,C_j)=dist(\mu_i,\mu_j)$，其中，$dist(·,·$)用于计算两个样本之间的距离；$\mu$代表簇$C$的中心点$\mu=\frac{1}{|C|}\sum_{1\le i\le|C|}x_i$。



**DB指数**：$DBI=\frac{1}{k}\sum\limits_{i=1}^k\mathop{max}\limits_{j\neq i}(\frac{abg(C_i+avg(C_j))}{d_{cen}(C_i,C_j)})$

**Dunn指数**：$DI=\mathop{min}\limits_{1\le i\le k}\{\mathop{min}\limits_{j\neq i}(\frac{d_{min(C_i,C_j)}}{\mathop{max}\limits_{1\le l\le k}diam(C_l)})\}$

$DBI$的值越小越好，而$DI$值越大越好。

## 距离计算

**闵可夫斯基距离**：$dist_{mk}(x_i,x_j)=(\sum\limits_{u=1}^n|x_{iu}-x_{ju}|^p)^\frac{1}{p}$，亦即$x_i-x_j$的$L_p$范数$||x_i-x_j||_p$。

对无序属性可采用$VDM$，令$m_{u,a}$表示在属性$u$上取值为$a$的样本数，$m_{u,a,i}$表示在第$i$个样本簇中在属性$u$上取值为$a$的样本数，$k$为样本簇数，则属性$u$上两个离散值$a$与$b$之间的$VDM$距离为$VDM_p(a,b)=\sum\limits_{i=1}^k|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}|^p$。于是，将闵可夫斯基距离和$VDM$结合即可处理混合属性，假定有$n_c$个有序属性、$n-n_c$个无序属性：$minkovDM_p(x_i,x_j)=(\sum\limits_{u=1}^{n_c}|x_{iu}-x_{ju}|^p+\sum\limits_{u=n_c+1}^nVDM_p(x_{iu},x_{ju}))^{\frac{1}{p}}$。

当样本空间中不同属性的重要性不同时，可使用”加权距离“，通常权重和为1。

需注意的是，通常我们是基于某种形式的距离来定义”相似度度量“，距离越大，相似度越小。然而用于相似度度量的距离未必一定要满足距离度量的所有基本性质，尤其是直递性。

## 原型聚类

### k均值算法

给定样本集$D=\{x_1,x_2,\cdots,x_m\}$，”k均值“算法针对聚类所得簇划分$C=\{C_1,C_2,\cdots,C_k\}$最小化平方误差$E=\sum\limits_{i=1}^k\sum\limits_{x\in C_i}||x-\mu_i||_2^2$，其中$\mu_i=\frac{1}{|C_i|}\sum\limits_{x\in C_i}x$是簇$C_i$的均值向量。$k$均值算法采用了贪心策略，通过迭代优化来近似求解。

### 学习向量量化

与一般聚类算法不同，$LVQ$假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。

给定样本集$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)\}$，每个样本$x_j$是由$n$个属性描述的特征向量$(x_{j1};x_{j2};\cdots;x_{jn})$，$y_j\in\gamma$是样本$x_j$的类别标记。$LVQ$的目标是学得一组$n$维原型向量$\{p_1,p_2,\cdots,p_q\}$，每个原型向量代表一个聚类簇，簇标记$t_i\in\gamma$。

### 高斯混合聚类

定义高斯混合分布$p_M(x)=\sum\limits_{i=1}^k\alpha_i·p(x|\mu_i,\sum_i)$，该分布共由$k$个混合成分组成，每个混合成分对应一个高斯分布。其中$\mu_i$与$\sum_i$是第$i$个高斯混合成分的参数，而$\alpha>0$维相应的”混合系数“，$\sum\limits_{i=1}^k\alpha_i=1$。

假设样本的生成过程由高斯混合分布给出：首先，根据$\alpha_1,\alpha_2,\cdots,\alpha_k$定义的先验分布选择高斯混合成分，其中$\alpha_i$为选择第$i$个混合成分的概率；然后，根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。当高斯混合分布已知时，高斯混合聚类将把样本集$D$划分为$k$个簇$C=\{C_1,C_2,\cdots,C_k\}$，每个样本$x_j$的簇标记$\lambda_j$通过$\lambda_j=\mathop{argmax}\limits_{i\in\{1,2,\cdots,k\}}\gamma_{ji}$。

## 密度聚类

密度聚类亦称”基于密度的聚类“，此类算法假设聚类结构能通过样本分布的紧密程度确定。通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。其中$DBSCAN$是一种著名的密度聚类算法，它基于一组”邻域“参数来刻画样本分布的紧密程度。

## 层次聚类

层次聚类试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用”自底向上“的聚合策略，也可采用”自顶向下“的分拆策略。其中$AGNES$是一种采用自底向上聚合策略的层次聚类算法。它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直到达到预设的聚类簇个数。
