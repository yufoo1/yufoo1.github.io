---
sidebar_position: 1
---
# 基本库的使用
## urllib
$urllib$是$Python$内置的$HTTP$请求库，它包含如下4个模块。
1. $request$：最基本的$HTTP$请求模块，可以用来模拟发送请求。
2. $error$：异常处理模块。
3. $parse$：工具模块，提供了许多$URL$方法，比如拆分、解析、合并等。
4. $robotparser$：主要用来识别网站的$robots.txt$文件。
### 发送请求
#### urlopen()
##### 返回值
它返回一个$HTTPResponse$类型的对象，主要包含$read()，readinto()，getheader(name)，getheaders()，fileno()$等方法，以及$msg、version、status、reason、debuglevel、closed$等属性。
##### 参数
1. $data$：如果添加该参数，需要使用$bytes()$方法将参数转化为字节流编码格式的内容，即$bytes$类型。$bytes()$方法的第一个参数需要是$str$（字符串）类型，需要用$urllib.parse$模块里的$urlencode()$方法来将参数字典转化为字符串；第二个参数指定编码格式。
```python
import urllib.parse
import urllib.request

data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding = 'utf-8')
response = urllib.request.urlopen('http://httpbin.org/post', data = data)
print(response.read())
```
2. $timeout$: $timeout$参数用于设置超时时间，单位为秒。
```python
import urllib.request

response = urllib.request.urlopen('http://httpbin.org/get', timeout = 1)
print(response.read())
```
```python
# 通过设置超时时间来控制一个网页在长时间未响应时，就跳过它的抓取
import socket
import urlliv.request
import urllib.error

try:
    response = urllib.request.urlopen('http://httpbin.org/get', timeout = 0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```
3. $context$：它必须是$ssl.SSLContext$类型，用来指定$SSL$设置。
4. $cafile$：指定$CA$证书。
5. $capath$：指定$CA$证书的路径。
#### Request
##### 参数
1. $url$：必传参数，用于请求$URL$。
2. $data$：$bytes$（字节流）类型，如果它是字典，可以先用$urllib.parse$模块里的$urlencode()$编码。
3. $headers$：字典类型，请求头。我们可以在构造请求时通过$headers$参数直接构造，也可以通过调用请求示例的$add_header()$方法添加。
4. $origin\_req\_host$：请求方的$host$名称或者$IP$地址。
5. $unverifiable$：表示这个请求是否是无法验证的。
6. $method$：字符串，用来指示请求使用的方法。
```python
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'Use-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}
dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict), encoding = 'utf8')
req = request.Request(url = url, data = data, headers = headers, method = 'POST')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```
```python
# 通过add_header()方法来添加headers
req = request.Request(url = url, data = data, method = 'POST')
req.add_header('User-Agent', 'Mozilla/4.0(compatible; MSIE 5.5; Window NT)')
```
#### 高级用法
##### BaseHandler类
$urllib.request$模块里的$BaseHandler$类，它是所有其他$Handler$的父类，提供了最基本的方法，例如$default\_open()$、$protocol\_request()$等。而有各种的$Handler$子类继承这个$BaseHandler$类，例如：
1. $HTTPDefaultErrorHandler$：用于处理$HTTP$响应错误，错误都会抛出$HTTPError$类型的异常。
2. $HTTPRedirectHandler$：用于处理重定向。
3. $HTTPCookieProcessor$：用于处理$Cookies$。
4. $ProxyHandler$：用于设置处理，默认处理为空。
5. $HTTPPasswordMgr$：用于管理密码，它维护了用户名和密码的表。
6. $HTTPBasicAuthHandler$：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。
##### OpenerDirector类
$OpenerDirector$类也可以称为$Opener$。$Opener$可以使用$open()$方法，返回值的类型与$urlopen()$相同，而我们可以利用$Handler$来构建$Opener$。
1. 验证
```python
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

username = 'username'
password = 'password'
url = 'http://localhost:5000/'

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)

try:
    result = opener.open(url)
    html = result,read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)
```
2. 代理
```python
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

proxy_handler = ProxyHandler({
    'http': 'http://127.0.0.1:9743',
    'https': 'http://127.0.0.1:9743'
})
opener = build_opener(proxy_handler)
try:
    response = opener.open('https://www.baidu.com')
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```
3. Cookies
```python
# 获取网站的Cookies并打印
import http.cookiejar, urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name+"="+item.value)
```
```python
# 获取网站的Cookies并输出成文件格式
filename = 'cookies.txt'
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.con')
cookie.save(ignore_discart = True, ignore_expires = True)

#若要保存成LWP格式的Cookies，可以在声明时改为：
cookie = http.cookiejar.LWPCookieJar(filename)
```
```python
# 从LWPCookieJar格式的文件中读取并利用Cookies
cookie = http.cookiejar.LWPCookieJar()
cookie.load('cookies.txt', ignore_discart = True, ignore_expires = True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
print(response.read().decode('utf-8'))
```
### 处理异常
#### URLError
$URLError$类来自$urllib$库的$error$模块，它继承自$OSError$类，是$error$异常模块的基类，由$request$模块产生的异常都可以通过捕获这个类来处理。$URLError$具有一个属性$reason$，即返回错误的原因。
```python
from urllib import request, error
try:
    response = request.urlopen('https://cuiqingcai.com/index.htm')
except error.URLError as e:
    print(e.reason)
```
#### HTTPError
$HTTPError$是$URLError$的子类，专门用来处理$HTTP$请求错误，比如认证请求失败等。它有如下3个属性。
1. $code$：返回$HTTP$状态码。
2. $reason$：同$URLError$一样，用于返回错误的原因。
3. $headers$：返回请求头。
```python
from urllib import request, error

try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep = '\n')
except error.URLError as e:
    print(e.reason)
else:
    print('Request Successfully')
```
```python
# reason属性返回的不一定是字符串，也可能是一个对象
import socket
import urllib.request
import urllib.error

try:
    response = urllib.request.urlopen('https://www.baidu.com', timout = 0.01)
except urllib.error.URLError as e:
    print(type(e.reason))
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```
### 解析链接
#### urlparse()
##### URL结构
```python
from urllib.parse import urlparse

result = urlparse('http://www.baidu.com/index.html;user?id=5#commet')
print(type(result), result)
```
标准的$URL$链接格式如下：
$scheme://nettloc/path;params?query$fragment$
其中，$scheme$代表协议；$netloc$代表域名；$path$代表访问路径；$params$代表参数，$query$代表查询条件，一般用作$GET$类型的$URL$；$fragment$是锚点，用于直接定位页面内部的下拉位置。
##### 参数
1. $urlstring$：待解析的$URL$。
2. $scheme$：默认的协议（比如$http$或$https$等）。
$scheme$参数只有在$URL$中不包含$scheme$信息时才生效，如果$URL$中有$scheme$信息，$urlparse()$就会返回解析出的$scheme$。
3. $allow_fragments$：是否忽略$fragment$，如果它被设置为$false$，$fragment$部分就会被忽略，它会被解析为$path$、$params$或者$query$的一部分，而$fragment$部分为空。
#### urlunparse()
$urlunparse()$接受的参数是一个可迭代对象，但长度必须是6。
```python
from urllib.parse import urlunparse

data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
# 这里参数data使用了列表类型，也可以使用其他类型，比如元组或者特定的数据结构
print(urlunparse(data))
```
#### urlsplit()
这个方法与$urlparse()$方法较为类似，只是其不再单独解析$params$部分，而仅返回5个结果，返回结果是一个元组类型。
#### urlunsplit()
这个方法与$urlunsplit()$方法较为类似，区别是长度必须为5。
#### urljoin()
我们可以提供一个$base\_url$（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析$base\_url$的$scheme、netloc$和$path$这3个内容并对新链接缺失的部分进行补充，最后返回结果。
如果$scheme、netloc$和$path$这三项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而$base\_url$中的$params、query$和$fragment$是不起作用的。
#### urlencode()
有时为了更加方便地构造参数，我们会事先用字典来表示。要转化为$URL$的参数时，只需要调用该方法即可。
```python
from urllib.parse import urlencode

paarams = {
    'name': 'germey',
    'age': 22
}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)
print(url)
```
#### parse_qs()
如果我们有一串$GET$请求参数，利用$parse_qs()$方法，就可以将它转回字典。
```python
from urllib.parse import parse_qs

query = 'name=germey&age=22'
print(parse_qs(query))
```
#### parse_qsl()
$parse_qsl$方法用于将参数转化为元组组成的列表。
```python
from urllib.parse import parse_qsl

query = 'name=germy&age=22'
print(parse_qsl(query))
# 运行结果是一个列表，而列表中的每一个元素都是一个元组，元组的第一个内容是参数，第二个内容是参数值
```
#### quote()
该方法可以将内容转化为$URL$编码的格式。$URL$种带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为$URL$编码。
```python
from urllib.parse import quote

keyword = '壁纸'
url = 'https://www.baidu.com/s?wd=' + quote(keyword)
print(url)
```
#### unquote()
```python
from urllib.parse import unquote

url = 'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%BB'
print(unquote(url))
```
### 分析Robots协议
#### robotparser
##### 声明
$urllib.robotparser.RobotFileParser(url='')$
##### 方法
1. $set\_url$：用来设置$robot.txt$文件的链接。如果在创建$RobotFileParser$对象时传入了链接，那么就不需要再使用这个方法设置了。
2. $read()$：读取$robots.txt$文件并进行分析，必须调用。
3. $parse()$：用来解析$robots.txt$文件，传入的参数是$robots.txt$某些行的内容，它会按照$robots.txt$的语法规则来分析这些内容。
4. $can\_fetch()$：该方法传入两个参数，第一个是$User-agent$，第二个是要抓取的$URL$。返回的内容是该搜索引擎是否可以抓取这个$URL$，返回结果是$True$或$False$。
5. $mtime()$：返回的是上次抓取和分析$robots.txt$的时间。
6. $modified()$：其将当前时间设置为上次抓取和分析$robots.txt$的时间。
```python
from urllib.robotparser import RobotFileParser

rp = RobotFileParser()
rp.set_url('http://www.jianshu.com/robots.txt')
rp.read()
print(rp.can_fetch('*', 'https://www,jianshu.com/p/b67554025d7d'))
```
```python
from urllib.robotparser import RobotFileParser
from urllib.request import urlopen

rp = RobotFileParser()
rp.parse(urlopen('http://www.jianshu.com/robots.txt').rear().decode('utf-8').split('\n'))
print(rp.can_fetch('*', 'https://www,jianshu.com/p/b67554025d7d'))
```
## requests
### GET请求
```python
import requests

data = {
    'name': 'germey',
    'age': 22
}
r = requests.get('http://httpbin.org/get', params = data)
# 请求的链接自动被构造成了：http://httpbin.org/get?age=22&name=germey。
# headers可以通过相同方法添加。
# 调用json()方法可以将返回结果是JSON格式的字符串转化为字典
# 大多网页请求返回的是一个HTML文档
print(r.text)
```
```python
# 抓取二进制数据
import requests

r = requests.get('http://github.com/favicon.ico')
with open('favicon.ico', 'wb') as f:
    f.write(r.content)
```
### POST请求
```python
import requests

data = {
    'name': 'germey',
    'age': 22
}
r = requests.post('http://httpbin.org/post', data = data)
print(r.text)
```
#### 响应
```python
import requests

r = requests.get('http://www.jianshu.com')
exit() if not r.status_code == requests.codes.ok else print('Request Successfully')
# 请求得到正常相应则输出成功请求的信息，否则程序终止。
```
### 高级用法
#### 文件上传
```python
import requests

files = {
    'file': open('favicon.ico', 'rb')
}
r = requests.post('http://httpbin.org/post', files = files)
print(r.text)
```
#### Cookies
```python
import requests

r = requests.get('http://www.baidu.com')
print(r.cookies)
for key, value in r.cookies.items():
    print(key + '=' + value)
```
$Cookie$可设置在$Headers$中，也可通过$get()$方法的$cookies$参数来设置，不过这样就需要构造$RequestsCookieJar$对象，而且需要分割一下$cookies$。
#### 会话维持
利用$Session$对象，可以用于模拟在一个浏览器中打开同一站点的不同页面。
```python
import reuqests

s = requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789')
r = s.get('http://httpbin.org/cookies')
print(r.text)
```
#### SSL证书验证
```python
import requests

response = requests.get('http://www.12306.cn', verify = False)
print(response.status_code)
# verify参数控制当发送HTTP请求的时候,是否检查SSL证书,默认是True
# 我们也可以指定一个本地证书用作客户端证书,这可以是单个文件(包含密钥和证书)或一个包含两个文件路径的元组
```
#### 代理设置
```python
import requests

proxies = {
    'http': 'http://10.10.1.10:3128',
    'https': 'http://10.10.1.10:1080'
}
requests.get('http://www.taobao.com', proxies = proxies)
```
#### 超时设置
```python
import requests

r = requests.get('http://www.taobao.com', timeout = 1)
print(r.status_code)
# 分别设置connect和read的timeout总和
r = requests.get('http://www.taobao.com', timeout = (5, 30))
```
#### 身份验证
```python
import requests
from requests.auth import HTTPBasicAuth

r = requests.get('http://localhost:5000', auth = HTTPBasicAuth('username', password))
print(r.status_code)
```
#### Prepared Request
```python
from requests import Request, Session

url = 'http://httpbin.org/post'
data = {
    'name': 'germey'
}
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'
}
s = Session()
req = Request('POST', url, data = data, headers = headers)
prepped = s.prepare_request(req)
r = s.send(prepped)
print(r.text)
```
## 正则表达式
### 常用的匹配规则
|模式|描述|
|:---:|---|
|\w|匹配字母、数字及下划线|
|\W|匹配不是字母、数字及下划线的字符|
|\s|匹配任意空白字符，等价于[\t\n\r\f]|
|\S|匹配任意非空字符|
|\d|匹配任意数字，等价于[0-9]|
|\D|匹配任意非数字的字符|
|\A|匹配字符串开头|
|\Z|匹配字符串结尾，如果存在换行，只匹配到换行前的结束字符串|
|\z|匹配字符串结尾，如果存在换行，同时还会匹配换行符|
|\G|匹配最后匹配完成的位置|
|\n|匹配一个换行符|
|\t|匹配一个制表符|
|^|匹配一行字符串的开头|
|$|匹配一行字符串的结尾|
|.|匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符|
|[...]|用来表示一组字符，单独列出，比如[amk]匹配a、m或k|
|[^...]|不在[]中的字符，比如[^abc]匹配除了a、b、c之外的字符|
|*|匹配0个或多个表达式|
|+|匹配1个或多个表达式|
|?|匹配0个或1个前面的正则表达式定义的片段，非贪婪方式|
|{n}|精确匹配n个前面的表达式|
|{n, m}|匹配n到m次由前面正则表达式定义的片段，贪婪方式|
|a\|b|匹配a或b|
|()|匹配括号内的表达式，也表示一个组|
### re
#### match()
$match()$方法从字符串的开头开始匹配，一旦开头不匹配，那么整个匹配就失败了。
```python
import re

content = 'Hello 123 4567 World_This is a Regex Demo'
print(len(content))
result = re.match('^Hello\s\d\d\d\s\d{4}\s\w{10}', content)
print(result)
print(result.group())
print(result.span())
```
```python
import re

content = 'Hello 1234567 World_This is a Regex Demo'
result = re.match('^Hello\s(\d+)\sWorld', content)
print(result)
print(result.group())
print(result.group(1))
print(result.span())
```
在贪婪匹配下，$.*$会匹配尽可能多的字符，而非贪婪匹配的写法是.*?。
##### 修饰符
|修饰符|描述|
|:---:|---|
|re.I|使匹配对大小不敏感|
|re.L|做本地化识别（locale-aware）匹配|
|re.M|多行匹配，影响^和$|
|re.S|使.匹配包括换行在内的所有字符|
|re.U|根据Unicode字符集解析字符。这个标志影响\w、\W、\b和\B|
|re.X|该标志通过给予更灵活的格式以便将正则表达式写得更易于理解|
##### 转义匹配
当遇到用于正则表达式匹配模式的特殊字符时，在前面加反斜线转义一下即可。
#### search()
$search()$方法在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果。
#### findall()
$findall()$方法会搜索整个字符串，然后以列表形式返回匹配正则表达式的所有内容，返回列表中的每个元素都是元组类型。
#### sub()
$sub()$方法用于修改文本。
```python
import re

content = '54aK54yr5oiR54ix5L2g'
content = re.sub('\d+', '', content)
# 第一个参数传入\d+来匹配所有的数字，第二个参数为替换成的字符串（如果去掉该参数的话，可以赋值为空），第三个参数是原字符串
print(content)
```
#### compile()
$compile()$方法可以将正则字符串编译成正则表达式对象，以便在后面的匹配中复用。$而compile()$还可以传入修饰符。
```python
import re

content = '2016-12-15 12: 00'
pattern = re.compile('\d{2}:\d{2}')
result = re.sub(pattern, '', content)
print(result)
```
